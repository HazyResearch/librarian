#!/usr/bin/env python
# librarian-publish -- publish a data set
# 
# Usage example:
# > librarian publish --project memex --name "First shipment of basic attributes" /lfs/local/0/mjc/foo/bar.csv
#

# TODO support Google Cloud Storage latttp://www.ncbi.nlm.nih.gov/pmc/tools/ftp/er
from storage_s3 import upload
from database_sheetsdb import insert
import argparse, sys, os, datetime, socket

parser = argparse.ArgumentParser()
parser.add_argument('local_path', nargs='+')
parser.add_argument('--type',    required=True, choices=['incoming', 'outgoing'])
parser.add_argument('--project', required=True)
parser.add_argument('--dataset', required=True)
parser.add_argument('--version')
parser.add_argument('--comments')
parser.add_argument('--metadata_url')
args = parser.parse_args()

print >>sys.stderr, args

# sanitize args
for p in args.local_path:
    if not os.path.exists(p):
        raise Exception(p + ': Invalid path')

# TODO decide remote path from project and name
# done inside storage_s3.upload
timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S%f')

# upload given files to AWS S3 or GCS
print >>sys.stderr, "Uploading files to S3..."
try:
    urls, checksums = upload(args.local_path, args.project, args.dataset, timestamp)
except:
    raise Exception('Something went wrong. Please try again.')

# finally, insert a record into Librarian's database
print >>sys.stderr, "Registering %d files to Librarian..." % len(urls)
table = 'IncomingData' if args.type == 'incoming' else 'OutgoingData'
insert(table,
        project=args.project,
        dataset=args.dataset,
        version=args.version,
        timestamp=timestamp,
        comments=args.comments,
        metadata_url=args.metadata_url,
        urls="\n".join(urls),
        checksums=checksums,
        user=os.getlogin(),
        hostname=socket.gethostname())

